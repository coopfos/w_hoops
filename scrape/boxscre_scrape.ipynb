{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HTML to: /users/cooperfoster/desktop/w_hoops/schedule_output.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ==== EDIT THESE ====\n",
    "OUT_PATH = \"/users/cooperfoster/desktop/w_hoops/schedule_output.txt\"\n",
    "# ====================\n",
    "\n",
    "URL = \"https://www.sports-reference.com/cbb/boxscores/index.cgi?month=12&day=13&year=2025\"\n",
    "\n",
    "# ~6 requests/minute pacing (not strictly needed for one page, but kept consistent)\n",
    "REQUESTS_PER_MIN = 6\n",
    "DELAY_SECONDS = 60 / REQUESTS_PER_MIN\n",
    "\n",
    "opts = webdriver.ChromeOptions()\n",
    "opts.add_argument(\"--headless=new\")\n",
    "opts.add_argument(\"--no-sandbox\")\n",
    "opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "\n",
    "try:\n",
    "    driver.get(URL)\n",
    "    time.sleep(3)  # allow dynamic content + comments to load\n",
    "\n",
    "    html = driver.page_source\n",
    "\n",
    "    os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "    with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "\n",
    "    print(f\"Saved HTML to: {OUT_PATH}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "time.sleep(DELAY_SECONDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-24 -> 29 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-01-24.txt\n",
      "[WARN] 2025-01-25 scrape took 71.0s (potentially slower than necessary)\n",
      "2025-01-25 -> 276 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-01-25.txt\n",
      "2025-01-26 -> 53 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-01-26.txt\n",
      "2025-01-27 -> 38 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-01-27.txt\n",
      "2025-01-28 -> 33 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-01-28.txt\n",
      "2025-01-29 -> 102 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-01-29.txt\n",
      "2025-01-30 -> 142 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-01-30.txt\n",
      "2025-01-31 -> 30 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-01-31.txt\n",
      "2025-02-01 -> 266 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-01.txt\n",
      "2025-02-02 -> 64 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-02.txt\n",
      "2025-02-03 -> 29 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-03.txt\n",
      "2025-02-04 -> 39 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-04.txt\n",
      "2025-02-05 -> 106 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-05.txt\n",
      "2025-02-06 -> 144 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-06.txt\n",
      "2025-02-07 -> 20 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-07.txt\n",
      "2025-02-08 -> 281 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-08.txt\n",
      "2025-02-09 -> 49 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-09.txt\n",
      "2025-02-10 -> 24 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-10.txt\n",
      "2025-02-11 -> 42 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-11.txt\n",
      "2025-02-12 -> 93 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-12.txt\n",
      "2025-02-13 -> 150 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-13.txt\n",
      "2025-02-14 -> 27 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-14.txt\n",
      "2025-02-15 -> 274 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-15.txt\n",
      "2025-02-16 -> 63 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-16.txt\n",
      "2025-02-17 -> 31 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-17.txt\n",
      "2025-02-18 -> 38 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-18.txt\n",
      "2025-02-19 -> 102 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-19.txt\n",
      "2025-02-20 -> 137 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-20.txt\n",
      "2025-02-21 -> 26 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-21.txt\n",
      "2025-02-22 -> 257 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-22.txt\n",
      "2025-02-23 -> 72 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-23.txt\n",
      "2025-02-24 -> 33 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-24.txt\n",
      "2025-02-25 -> 43 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-25.txt\n",
      "2025-02-26 -> 106 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-26.txt\n",
      "2025-02-27 -> 148 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-27.txt\n",
      "2025-02-28 -> 43 blocks -> /users/cooperfoster/desktop/w_hoops/schedule_links/2025-02-28.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ==== EDIT THESE ====\n",
    "OUT_DIR = \"/users/cooperfoster/desktop/w_hoops/schedule_links\"\n",
    "START = date(2025, 1, 24)\n",
    "END   = date(2025, 2, 28)\n",
    "TARGET_CLASS = \"game_summary\"  # matches <div class=\"game_summary ...\">  [oai_citation:1â€¡html_sample.txt](sediment://file_00000000e42471fda9fdb2be1566e777)\n",
    "# ====================\n",
    "\n",
    "BASE_URL = \"https://www.sports-reference.com/cbb/boxscores/index.cgi?month={m}&day={d}&year={y}\"\n",
    "\n",
    "# Rolling-window rate limit: max 6 requests in any 60-second window\n",
    "MAX_REQ = 6\n",
    "WINDOW_SEC = 60\n",
    "req_times = deque()  # timestamps of request starts\n",
    "\n",
    "def rate_limit():\n",
    "    now = time.time()\n",
    "    while req_times and (now - req_times[0]) >= WINDOW_SEC:\n",
    "        req_times.popleft()\n",
    "    if len(req_times) >= MAX_REQ:\n",
    "        sleep_for = WINDOW_SEC - (now - req_times[0]) + 0.05\n",
    "        time.sleep(max(0, sleep_for))\n",
    "        # clean again after sleeping\n",
    "        now = time.time()\n",
    "        while req_times and (now - req_times[0]) >= WINDOW_SEC:\n",
    "            req_times.popleft()\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "opts = webdriver.ChromeOptions()\n",
    "opts.add_argument(\"--headless=new\")\n",
    "opts.add_argument(\"--no-sandbox\")\n",
    "opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "\n",
    "try:\n",
    "    cur = START\n",
    "    while cur <= END:\n",
    "        rate_limit()\n",
    "        req_times.append(time.time())\n",
    "\n",
    "        url = BASE_URL.format(m=cur.month, d=cur.day, y=cur.year)\n",
    "        out_path = os.path.join(OUT_DIR, f\"{cur.isoformat()}.txt\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for either game summaries or a \"no games\" style page; don't hard-sleep.\n",
    "        try:\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.any_of(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, f\"div.{TARGET_CLASS}\")),\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.game_summaries\")),\n",
    "                    EC.presence_of_element_located((By.ID, \"content\"))\n",
    "                )\n",
    "            )\n",
    "        except Exception:\n",
    "            pass  # still try to parse whatever loaded\n",
    "\n",
    "        html = driver.page_source\n",
    "        elapsed = time.time() - t0\n",
    "        if elapsed > 10:\n",
    "            print(f\"[WARN] {cur.isoformat()} scrape took {elapsed:.1f}s (potentially slower than necessary)\")\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        blocks = soup.find_all(\"div\", class_=lambda c: c and TARGET_CLASS in c.split())\n",
    "\n",
    "        # Write only the extracted class blocks (not the whole HTML)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            if not blocks:\n",
    "                f.write(\"\")  # keep empty file to mark the date was attempted\n",
    "            else:\n",
    "                for i, div in enumerate(blocks, start=1):\n",
    "                    f.write(f\"<!-- {TARGET_CLASS} #{i} | {cur.isoformat()} -->\\n\")\n",
    "                    f.write(str(div))\n",
    "                    f.write(\"\\n\\n\")\n",
    "\n",
    "        print(f\"{cur.isoformat()} -> {len(blocks)} blocks -> {out_path}\")\n",
    "\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 117 files -> 10724 games\n",
      "Saved: /users/cooperfoster/desktop/w_hoops/game_links.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_date</th>\n",
       "      <th>gender</th>\n",
       "      <th>winner_sid</th>\n",
       "      <th>loser_sid</th>\n",
       "      <th>boxscore_href</th>\n",
       "      <th>boxscore_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>women</td>\n",
       "      <td>south-carolina</td>\n",
       "      <td>michigan</td>\n",
       "      <td>/cbb/boxscores/2024-11-04-19-south-carolina_w....</td>\n",
       "      <td>https://www.sports-reference.com/cbb/boxscores...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>men</td>\n",
       "      <td>kansas</td>\n",
       "      <td>howard</td>\n",
       "      <td>/cbb/boxscores/2024-11-04-20-kansas.html</td>\n",
       "      <td>https://www.sports-reference.com/cbb/boxscores...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>men</td>\n",
       "      <td>alabama</td>\n",
       "      <td>north-carolina-asheville</td>\n",
       "      <td>/cbb/boxscores/2024-11-04-21-alabama.html</td>\n",
       "      <td>https://www.sports-reference.com/cbb/boxscores...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>women</td>\n",
       "      <td>southern-california</td>\n",
       "      <td>mississippi</td>\n",
       "      <td>/cbb/boxscores/2024-11-04-12-mississippi_w.html</td>\n",
       "      <td>https://www.sports-reference.com/cbb/boxscores...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>men</td>\n",
       "      <td>houston</td>\n",
       "      <td>jackson-state</td>\n",
       "      <td>/cbb/boxscores/2024-11-04-20-houston.html</td>\n",
       "      <td>https://www.sports-reference.com/cbb/boxscores...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    game_date gender           winner_sid                 loser_sid  \\\n",
       "0  2024-11-04  women       south-carolina                  michigan   \n",
       "1  2024-11-04    men               kansas                    howard   \n",
       "2  2024-11-04    men              alabama  north-carolina-asheville   \n",
       "3  2024-11-04  women  southern-california               mississippi   \n",
       "4  2024-11-04    men              houston             jackson-state   \n",
       "\n",
       "                                       boxscore_href  \\\n",
       "0  /cbb/boxscores/2024-11-04-19-south-carolina_w....   \n",
       "1           /cbb/boxscores/2024-11-04-20-kansas.html   \n",
       "2          /cbb/boxscores/2024-11-04-21-alabama.html   \n",
       "3    /cbb/boxscores/2024-11-04-12-mississippi_w.html   \n",
       "4          /cbb/boxscores/2024-11-04-20-houston.html   \n",
       "\n",
       "                                        boxscore_url  \n",
       "0  https://www.sports-reference.com/cbb/boxscores...  \n",
       "1  https://www.sports-reference.com/cbb/boxscores...  \n",
       "2  https://www.sports-reference.com/cbb/boxscores...  \n",
       "3  https://www.sports-reference.com/cbb/boxscores...  \n",
       "4  https://www.sports-reference.com/cbb/boxscores...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ==== EDIT THESE ====\n",
    "INPUT_DIR  = \"/users/cooperfoster/desktop/w_hoops/schedule_links\"       # folder containing YYYY-MM-DD.txt files\n",
    "OUTPUT_CSV = \"/users/cooperfoster/desktop/w_hoops/game_links.csv\"\n",
    "BASE = \"https://www.sports-reference.com\"\n",
    "# ====================\n",
    "\n",
    "sid_re = re.compile(r\"/cbb/schools/([^/]+)/\")\n",
    "date_re = re.compile(r\"(\\d{4}-\\d{2}-\\d{2})\")\n",
    "\n",
    "def extract_sid(a_tag):\n",
    "    if not a_tag:\n",
    "        return None\n",
    "    href = a_tag.get(\"href\")\n",
    "    if not href:\n",
    "        return None\n",
    "    m = sid_re.search(href)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def infer_gender(game_div):\n",
    "    # primary: gender-f / gender-m on the div itself\n",
    "    classes = set(game_div.get(\"class\", []))\n",
    "    if \"gender-f\" in classes:\n",
    "        return \"women\"\n",
    "    if \"gender-m\" in classes:\n",
    "        return \"men\"\n",
    "    # fallback: desc row text (\"Women's\"/\"Men's\")\n",
    "    desc = game_div.select_one(\"td.desc\")\n",
    "    if desc:\n",
    "        t = desc.get_text(\" \", strip=True).lower()\n",
    "        if \"women\" in t:\n",
    "            return \"women\"\n",
    "        if \"men\" in t:\n",
    "            return \"men\"\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "\n",
    "txt_paths = sorted(glob.glob(os.path.join(INPUT_DIR, \"*.txt\")))\n",
    "for path in txt_paths:\n",
    "    fname = os.path.basename(path)\n",
    "    mdate = date_re.search(fname)\n",
    "    game_date = mdate.group(1) if mdate else None  # expected from filename like 2025-01-24.txt\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw = f.read().strip()\n",
    "\n",
    "    if not raw:\n",
    "        continue\n",
    "\n",
    "    # Wrap fragments so BeautifulSoup always has a root\n",
    "    soup = BeautifulSoup(f\"<html><body>{raw}</body></html>\", \"lxml\")\n",
    "\n",
    "    for div in soup.select(\"div.game_summary\"):\n",
    "        gender = infer_gender(div)\n",
    "\n",
    "        w_tr = div.select_one(\"tr.winner\")\n",
    "        l_tr = div.select_one(\"tr.loser\")\n",
    "\n",
    "        winner_sid = extract_sid(w_tr.select_one(\"a\") if w_tr else None)\n",
    "        loser_sid  = extract_sid(l_tr.select_one(\"a\") if l_tr else None)\n",
    "\n",
    "        # Boxscore link is in td.gamelink a (can appear on either team row depending on ordering)\n",
    "        box_a = div.select_one(\"td.gamelink a\")\n",
    "        box_href = box_a.get(\"href\") if box_a else None\n",
    "        box_url = (BASE + box_href) if box_href else None\n",
    "\n",
    "        # Keep rows even if one team lacks a link (some non-D1 opponents show no href)\n",
    "        rows.append({\n",
    "            \"game_date\": game_date,\n",
    "            \"gender\": gender,                 # \"men\" / \"women\"\n",
    "            \"winner_sid\": winner_sid,\n",
    "            \"loser_sid\": loser_sid,\n",
    "            \"boxscore_href\": box_href,        # relative\n",
    "            \"boxscore_url\": box_url           # absolute\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Basic cleanup: drop rows with no boxscore link (should be rare, but safe)\n",
    "df = df[df[\"boxscore_href\"].notna()].reset_index(drop=True)\n",
    "\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Parsed {len(txt_paths)} files -> {len(df)} games\")\n",
    "print(f\"Saved: {OUTPUT_CSV}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2024-11-04-19-south-carolina_w -> wrote available tables in /users/cooperfoster/desktop/w_hoops/box scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2024-11-04-12-mississippi_w -> wrote available tables in /users/cooperfoster/desktop/w_hoops/box scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2024-11-04-14-louisville_w -> wrote available tables in /users/cooperfoster/desktop/w_hoops/box scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2024-11-04-17-notre-dame_w -> wrote available tables in /users/cooperfoster/desktop/w_hoops/box scores\n",
      "[WARN] 2024-11-04-20-louisiana-state_w: request took 69.7s (might be slower than necessary)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2024-11-04-20-louisiana-state_w -> wrote available tables in /users/cooperfoster/desktop/w_hoops/box scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2024-11-04-12-iowa-state_w -> wrote available tables in /users/cooperfoster/desktop/w_hoops/box scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2024-11-04-18-oklahoma_w -> wrote available tables in /users/cooperfoster/desktop/w_hoops/box scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2024-11-04-11-duke_w -> wrote available tables in /users/cooperfoster/desktop/w_hoops/box scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2024-11-04-12-kansas-state_w -> wrote available tables in /users/cooperfoster/desktop/w_hoops/box scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2024-11-04-11-north-carolina_w -> wrote available tables in /users/cooperfoster/desktop/w_hoops/box scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/2825423345.py:61: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2024-11-04-19-maryland-baltimore-county_w -> wrote available tables in /users/cooperfoster/desktop/w_hoops/box scores\n"
     ]
    },
    {
     "ename": "ReadTimeoutError",
     "evalue": "HTTPConnectionPool(host='localhost', port=52082): Read timed out. (read timeout=120)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1427\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[31mTimeoutError\u001b[39m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    119\u001b[39m _req_times.append(time.time())\n\u001b[32m    121\u001b[39m t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# wait for page to load; do not hard-sleep\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:483\u001b[39m, in \u001b[36mWebDriver.get\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    tab.\u001b[39;00m\n\u001b[32m    468\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m \u001b[33;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:455\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    452\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m    453\u001b[39m         params[\u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.session_id\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m response = \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRemoteConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mself\u001b[39m.error_handler.check_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/selenium/webdriver/remote/remote_connection.py:405\u001b[39m, in \u001b[36mRemoteConnection.execute\u001b[39m\u001b[34m(self, command, params)\u001b[39m\n\u001b[32m    403\u001b[39m trimmed = \u001b[38;5;28mself\u001b[39m._trim_large_entries(params)\n\u001b[32m    404\u001b[39m LOGGER.debug(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, command_info[\u001b[32m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/selenium/webdriver/remote/remote_connection.py:429\u001b[39m, in \u001b[36mRemoteConnection._request\u001b[39m\u001b[34m(self, method, url, body)\u001b[39m\n\u001b[32m    426\u001b[39m     body = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client_config.keep_alive:\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m     statuscode = response.status\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/urllib3/_request_methods.py:143\u001b[39m, in \u001b[36mRequestMethods.request\u001b[39m\u001b[34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request_encode_url(\n\u001b[32m    136\u001b[39m         method,\n\u001b[32m    137\u001b[39m         url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m         **urlopen_kw,\n\u001b[32m    141\u001b[39m     )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43murlopen_kw\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/urllib3/_request_methods.py:278\u001b[39m, in \u001b[36mRequestMethods.request_encode_body\u001b[39m\u001b[34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[39m\n\u001b[32m    274\u001b[39m     extra_kw[\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m].setdefault(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m, content_type)\n\u001b[32m    276\u001b[39m extra_kw.update(urlopen_kw)\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/urllib3/poolmanager.py:459\u001b[39m, in \u001b[36mPoolManager.urlopen\u001b[39m\u001b[34m(self, method, url, redirect, **kw)\u001b[39m\n\u001b[32m    457\u001b[39m     response = conn.urlopen(method, url, **kw)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m redirect_location = redirect \u001b[38;5;129;01mand\u001b[39;00m response.get_redirect_location()\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_e, (\u001b[38;5;167;01mOSError\u001b[39;00m, HTTPException)):\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n\u001b[32m    846\u001b[39m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_read_error(error):\n\u001b[32m    472\u001b[39m     \u001b[38;5;66;03m# Read retry?\u001b[39;00m\n\u001b[32m    473\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    476\u001b[39m         read -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/urllib3/util/util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value.__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     41\u001b[39m     value = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/urllib3/connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    534\u001b[39m     response = conn.getresponse()\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# Set properties that are used by the pooling layer.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/urllib3/connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Is the error actually a timeout? Will raise a ReadTimeout or pass\"\"\"\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(err, \u001b[33m\"\u001b[39m\u001b[33merrno\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m err.errno \u001b[38;5;129;01min\u001b[39;00m _blocking_errnos:\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPConnectionPool(host='localhost', port=52082): Read timed out. (read timeout=120)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ===== EDIT THESE =====\n",
    "INPUT_CSV = \"/users/cooperfoster/desktop/w_hoops/games_w.csv\"   # csv you generated (must include boxscore_url, winner_sid, loser_sid, gender)\n",
    "OUT_DIR   = \"/users/cooperfoster/desktop/w_hoops/box scores\"       # folder to write 4 csvs per game\n",
    "HEADLESS  = True\n",
    "# ======================\n",
    "\n",
    "# Rolling-window rate limit: max 6 requests in any 60-second window\n",
    "MAX_REQ = 6\n",
    "WINDOW_SEC = 60\n",
    "_req_times = deque()\n",
    "\n",
    "def rate_limit():\n",
    "    now = time.time()\n",
    "    while _req_times and (now - _req_times[0]) >= WINDOW_SEC:\n",
    "        _req_times.popleft()\n",
    "    if len(_req_times) >= MAX_REQ:\n",
    "        sleep_for = WINDOW_SEC - (now - _req_times[0]) + 0.05\n",
    "        time.sleep(max(0, sleep_for))\n",
    "        now = time.time()\n",
    "        while _req_times and (now - _req_times[0]) >= WINDOW_SEC:\n",
    "            _req_times.popleft()\n",
    "\n",
    "def game_id_from_url(url: str) -> str:\n",
    "    # e.g. .../cbb/boxscores/2025-01-24-michigan.html -> 2025-01-24-michigan\n",
    "    p = urlparse(url).path\n",
    "    base = os.path.basename(p)\n",
    "    return os.path.splitext(base)[0] or \"game\"\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "\n",
    "def extract_table_df(html: str, table_id: str):\n",
    "    \"\"\"\n",
    "    Sports-Reference often wraps tables in HTML comments.\n",
    "    This finds the table either directly or inside comments and returns a DataFrame.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # 1) Direct table\n",
    "    tbl = soup.find(\"table\", id=table_id)\n",
    "    if tbl is not None:\n",
    "        return pd.read_html(str(tbl))[0]\n",
    "\n",
    "    # 2) Comment-wrapped table\n",
    "    for c in soup.find_all(string=lambda t: isinstance(t, Comment)):\n",
    "        if table_id in c:\n",
    "            csoup = BeautifulSoup(c, \"lxml\")\n",
    "            tbl2 = csoup.find(\"table\", id=table_id)\n",
    "            if tbl2 is not None:\n",
    "                return pd.read_html(str(tbl2))[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "games = pd.read_csv(INPUT_CSV, dtype=str)\n",
    "required = {\"boxscore_url\", \"winner_sid\", \"loser_sid\"}\n",
    "missing_cols = required - set(games.columns)\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"INPUT_CSV missing required columns: {sorted(missing_cols)}\")\n",
    "\n",
    "# gender expected values: \"women\"/\"men\" (if absent, default to women-table ids with _w; you can change default)\n",
    "if \"gender\" not in games.columns:\n",
    "    games[\"gender\"] = \"women\"\n",
    "\n",
    "driver = init_driver(headless=HEADLESS)\n",
    "\n",
    "try:\n",
    "    for i, row in games.iterrows():\n",
    "        url = (row.get(\"boxscore_url\") or \"\").strip()\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        winner_sid = (row.get(\"winner_sid\") or \"\").strip()\n",
    "        loser_sid  = (row.get(\"loser_sid\") or \"\").strip()\n",
    "        gender     = (row.get(\"gender\") or \"women\").strip().lower()\n",
    "\n",
    "        if not winner_sid or not loser_sid:\n",
    "            print(f\"[SKIP] row {i}: missing winner/loser sid\")\n",
    "            continue\n",
    "\n",
    "        gid = game_id_from_url(url)\n",
    "\n",
    "        # table id suffix logic: women pages usually end with _w; men usually have no _w\n",
    "        suffix = \"_w\" if gender.startswith(\"w\") else \"\"\n",
    "\n",
    "        sids = [winner_sid, loser_sid]\n",
    "        table_specs = []\n",
    "        for sid in sids:\n",
    "            table_specs.append((f\"box-score-basic-{sid}{suffix}\",  f\"{gid}_{sid}_basic.csv\"))\n",
    "            table_specs.append((f\"box-score-advanced-{sid}{suffix}\", f\"{gid}_{sid}_advanced.csv\"))\n",
    "\n",
    "        out_paths = [os.path.join(OUT_DIR, fn) for _, fn in table_specs]\n",
    "        if all(os.path.exists(p) for p in out_paths):\n",
    "            # already have all four\n",
    "            continue\n",
    "\n",
    "        # rate limit only when needed (rolling window)\n",
    "        rate_limit()\n",
    "        _req_times.append(time.time())\n",
    "\n",
    "        t0 = time.time()\n",
    "        driver.get(url)\n",
    "\n",
    "        # wait for page to load; do not hard-sleep\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.ID, \"content\"))\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        html = driver.page_source\n",
    "        elapsed = time.time() - t0\n",
    "        if elapsed > 10:\n",
    "            print(f\"[WARN] {gid}: request took {elapsed:.1f}s (might be slower than necessary)\")\n",
    "\n",
    "        # extract + save four tables\n",
    "        for table_id, filename in table_specs:\n",
    "            out_path = os.path.join(OUT_DIR, filename)\n",
    "            if os.path.exists(out_path):\n",
    "                continue\n",
    "\n",
    "            df = extract_table_df(html, table_id)\n",
    "            if df is None:\n",
    "                print(f\"[MISS] {gid}: table not found -> #{table_id}\")\n",
    "                continue\n",
    "\n",
    "            df.to_csv(out_path, index=False)\n",
    "\n",
    "        print(f\"[OK] {gid} -> wrote available tables in {OUT_DIR}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ===== EDIT THESE =====\n",
    "INPUT_CSV = \"/users/cooperfoster/desktop/w_hoops/games_w.csv\"   # csv you generated (must include boxscore_url, winner_sid, loser_sid, gender)\n",
    "OUT_DIR   = \"/users/cooperfoster/desktop/w_hoops/box scores\" \n",
    "HEADLESS  = True\n",
    "# ======================\n",
    "\n",
    "# Rolling-window rate limit: max 6 requests in any 60-second window\n",
    "MAX_REQ = 6\n",
    "WINDOW_SEC = 60\n",
    "_req_times = deque()\n",
    "\n",
    "# Timeouts / retries\n",
    "PAGELOAD_TIMEOUT_SEC = 90   # per your request (was 120)\n",
    "WAIT_FOR_CONTENT_SEC = 30   # DOM readiness wait (not page-load timeout)\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def s(x):\n",
    "    \"\"\"Safe string: NaN/None -> '', else stripped string.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)) or pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "def rate_limit():\n",
    "    now = time.time()\n",
    "    while _req_times and (now - _req_times[0]) >= WINDOW_SEC:\n",
    "        _req_times.popleft()\n",
    "    if len(_req_times) >= MAX_REQ:\n",
    "        sleep_for = WINDOW_SEC - (now - _req_times[0]) + 0.05\n",
    "        time.sleep(max(0, sleep_for))\n",
    "        now = time.time()\n",
    "        while _req_times and (now - _req_times[0]) >= WINDOW_SEC:\n",
    "            _req_times.popleft()\n",
    "\n",
    "def game_id_from_url(url: str) -> str:\n",
    "    p = urlparse(url).path\n",
    "    base = os.path.basename(p)\n",
    "    return os.path.splitext(base)[0] or \"game\"\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    driver.set_page_load_timeout(PAGELOAD_TIMEOUT_SEC)\n",
    "    return driver\n",
    "\n",
    "def extract_table_df(html: str, table_id: str):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    tbl = soup.find(\"table\", id=table_id)\n",
    "    if tbl is not None:\n",
    "        return pd.read_html(str(tbl))[0]\n",
    "\n",
    "    for c in soup.find_all(string=lambda t: isinstance(t, Comment)):\n",
    "        if table_id in c:\n",
    "            csoup = BeautifulSoup(c, \"lxml\")\n",
    "            tbl2 = csoup.find(\"table\", id=table_id)\n",
    "            if tbl2 is not None:\n",
    "                return pd.read_html(str(tbl2))[0]\n",
    "    return None\n",
    "\n",
    "def fetch_html_with_retries(driver, url: str, gid: str):\n",
    "    \"\"\"\n",
    "    Fetch page HTML with automatic retries on page-load timeouts and transient webdriver errors.\n",
    "    Does not terminate the job; returns None if all retries fail.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            rate_limit()\n",
    "            _req_times.append(time.time())\n",
    "\n",
    "            t0 = time.time()\n",
    "            driver.get(url)\n",
    "\n",
    "            try:\n",
    "                WebDriverWait(driver, WAIT_FOR_CONTENT_SEC).until(\n",
    "                    EC.presence_of_element_located((By.ID, \"content\"))\n",
    "                )\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            elapsed = time.time() - t0\n",
    "            if elapsed > 10:\n",
    "                print(f\"[WARN] {gid}: request took {elapsed:.1f}s (potentially slower than necessary)\")\n",
    "\n",
    "            return driver.page_source\n",
    "\n",
    "        except TimeoutException as e:\n",
    "            last_err = e\n",
    "            print(f\"[RETRY] {gid}: page-load timeout (attempt {attempt}/{MAX_RETRIES})\")\n",
    "            try:\n",
    "                driver.execute_script(\"window.stop();\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(min(2 ** attempt, 8))\n",
    "\n",
    "        except WebDriverException as e:\n",
    "            last_err = e\n",
    "            msg = str(e).lower()\n",
    "            # Treat these as transient read/connection style failures\n",
    "            if (\"timeout\" in msg) or (\"timed out\" in msg) or (\"read\" in msg) or (\"disconnected\" in msg) or (\"connection\" in msg):\n",
    "                print(f\"[RETRY] {gid}: webdriver error (attempt {attempt}/{MAX_RETRIES}) -> {type(e).__name__}\")\n",
    "                time.sleep(min(2 ** attempt, 8))\n",
    "            else:\n",
    "                print(f\"[FAIL] {gid}: non-retryable webdriver error -> {type(e).__name__}: {e}\")\n",
    "                return None\n",
    "\n",
    "    print(f\"[SKIP] {gid}: failed after {MAX_RETRIES} retries -> {type(last_err).__name__ if last_err else 'Unknown'}\")\n",
    "    return None\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "games = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=True)\n",
    "games[\"winner_sid\"] = games[\"winner_sid\"].apply(s)\n",
    "games[\"loser_sid\"]  = games[\"loser_sid\"].apply(s)\n",
    "games[\"boxscore_url\"] = games[\"boxscore_url\"].apply(s)\n",
    "if \"gender\" in games.columns:\n",
    "    games[\"gender\"] = games[\"gender\"].apply(lambda x: s(x).lower())\n",
    "else:\n",
    "    games[\"gender\"] = \"women\"\n",
    "\n",
    "games = games[(games[\"boxscore_url\"] != \"\") & (games[\"winner_sid\"] != \"\") & (games[\"loser_sid\"] != \"\")]\n",
    "required = {\"boxscore_url\", \"winner_sid\", \"loser_sid\"}\n",
    "missing_cols = required - set(games.columns)\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"INPUT_CSV missing required columns: {sorted(missing_cols)}\")\n",
    "\n",
    "if \"gender\" not in games.columns:\n",
    "    games[\"gender\"] = \"women\"\n",
    "\n",
    "driver = init_driver(headless=HEADLESS)\n",
    "\n",
    "try:\n",
    "    for i, row in games.iterrows():\n",
    "        url        = s(row.get(\"boxscore_url\"))\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        winner_sid = s(row.get(\"winner_sid\"))\n",
    "        loser_sid  = s(row.get(\"loser_sid\"))\n",
    "        gender     = s(row.get(\"gender\")).lower() or \"women\"\n",
    "\n",
    "        if not winner_sid or not loser_sid:\n",
    "            print(f\"[SKIP] row {i}: missing winner/loser sid\")\n",
    "            continue\n",
    "\n",
    "        gid = game_id_from_url(url)\n",
    "\n",
    "        suffix = \"_w\" if gender.startswith(\"w\") else \"\"\n",
    "\n",
    "        sids = [winner_sid, loser_sid]\n",
    "        table_specs = []\n",
    "        for sid in sids:\n",
    "            table_specs.append((f\"box-score-basic-{sid}{suffix}\",   f\"{gid}_{sid}_basic.csv\"))\n",
    "            table_specs.append((f\"box-score-advanced-{sid}{suffix}\", f\"{gid}_{sid}_advanced.csv\"))\n",
    "\n",
    "        out_paths = [os.path.join(OUT_DIR, fn) for _, fn in table_specs]\n",
    "        if all(os.path.exists(p) for p in out_paths):\n",
    "            continue\n",
    "\n",
    "        html = fetch_html_with_retries(driver, url, gid)\n",
    "        if html is None:\n",
    "            continue  # move to next game without terminating the job\n",
    "\n",
    "        for table_id, filename in table_specs:\n",
    "            out_path = os.path.join(OUT_DIR, filename)\n",
    "            if os.path.exists(out_path):\n",
    "                continue\n",
    "\n",
    "            df = extract_table_df(html, table_id)\n",
    "            if df is None:\n",
    "                print(f\"[MISS] {gid}: table not found -> #{table_id}\")\n",
    "                continue\n",
    "\n",
    "            df.to_csv(out_path, index=False)\n",
    "\n",
    "        print(f\"[OK] {gid} -> wrote available tables in {OUT_DIR}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2025-02-28-19-william-mary_w -> wrote 4 tables (for 2 valid sid(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2025-02-28-20-dartmouth_w -> wrote 4 tables (for 2 valid sid(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n",
      "/var/folders/b0/c8j4grh117q9qb_7p60ysmbh0000gn/T/ipykernel_40784/267189997.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  return pd.read_html(str(tbl))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 2025-02-28-20-harvard_w -> wrote 4 tables (for 2 valid sid(s))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ===== EDIT THESE =====\n",
    "INPUT_CSV = \"/users/cooperfoster/desktop/w_hoops/games_w.csv\"   # csv you generated (must include boxscore_url, winner_sid, loser_sid, gender)\n",
    "OUT_DIR   = \"/users/cooperfoster/desktop/w_hoops/box scores\" \n",
    "HEADLESS  = True\n",
    "# ======================\n",
    "\n",
    "# Rolling-window rate limit: max 6 requests in any 60-second window\n",
    "MAX_REQ = 6\n",
    "WINDOW_SEC = 60\n",
    "_req_times = deque()\n",
    "\n",
    "# Timeouts / retries\n",
    "PAGELOAD_TIMEOUT_SEC = 90\n",
    "WAIT_FOR_CONTENT_SEC = 30\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "CLEAR_EVERY_N_LINES = 20\n",
    "_print_lines = 0\n",
    "\n",
    "def log(msg: str):\n",
    "    \"\"\"Print and periodically clear notebook cell output.\"\"\"\n",
    "    global _print_lines\n",
    "    print(msg)\n",
    "    _print_lines += 1\n",
    "    if _print_lines % CLEAR_EVERY_N_LINES == 0:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "def s(x):\n",
    "    \"\"\"Safe string: NaN/None -> '', else stripped string.\"\"\"\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(x).strip()\n",
    "\n",
    "def rate_limit():\n",
    "    now = time.time()\n",
    "    while _req_times and (now - _req_times[0]) >= WINDOW_SEC:\n",
    "        _req_times.popleft()\n",
    "    if len(_req_times) >= MAX_REQ:\n",
    "        sleep_for = WINDOW_SEC - (now - _req_times[0]) + 0.05\n",
    "        time.sleep(max(0, sleep_for))\n",
    "        now = time.time()\n",
    "        while _req_times and (now - _req_times[0]) >= WINDOW_SEC:\n",
    "            _req_times.popleft()\n",
    "\n",
    "def game_id_from_url(url: str) -> str:\n",
    "    p = urlparse(url).path\n",
    "    base = os.path.basename(p)\n",
    "    return os.path.splitext(base)[0] or \"game\"\n",
    "\n",
    "def init_driver(headless=True):\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    driver.set_page_load_timeout(PAGELOAD_TIMEOUT_SEC)\n",
    "    return driver\n",
    "\n",
    "def extract_table_df(html: str, table_id: str):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    tbl = soup.find(\"table\", id=table_id)\n",
    "    if tbl is not None:\n",
    "        return pd.read_html(str(tbl))[0]\n",
    "\n",
    "    for c in soup.find_all(string=lambda t: isinstance(t, Comment)):\n",
    "        if table_id in c:\n",
    "            csoup = BeautifulSoup(c, \"lxml\")\n",
    "            tbl2 = csoup.find(\"table\", id=table_id)\n",
    "            if tbl2 is not None:\n",
    "                return pd.read_html(str(tbl2))[0]\n",
    "    return None\n",
    "\n",
    "def fetch_html_with_retries(driver, url: str, gid: str):\n",
    "    last_err = None\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            rate_limit()\n",
    "            _req_times.append(time.time())\n",
    "\n",
    "            t0 = time.time()\n",
    "            driver.get(url)\n",
    "\n",
    "            try:\n",
    "                WebDriverWait(driver, WAIT_FOR_CONTENT_SEC).until(\n",
    "                    EC.presence_of_element_located((By.ID, \"content\"))\n",
    "                )\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            elapsed = time.time() - t0\n",
    "            if elapsed > 10:\n",
    "                log(f\"[WARN] {gid}: request took {elapsed:.1f}s (potentially slower than necessary)\")\n",
    "\n",
    "            return driver.page_source\n",
    "\n",
    "        except TimeoutException as e:\n",
    "            last_err = e\n",
    "            log(f\"[RETRY] {gid}: page-load timeout (attempt {attempt}/{MAX_RETRIES})\")\n",
    "            try:\n",
    "                driver.execute_script(\"window.stop();\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(min(2 ** attempt, 8))\n",
    "\n",
    "        except WebDriverException as e:\n",
    "            last_err = e\n",
    "            msg = str(e).lower()\n",
    "            if (\"timeout\" in msg) or (\"timed out\" in msg) or (\"read\" in msg) or (\"disconnected\" in msg) or (\"connection\" in msg):\n",
    "                log(f\"[RETRY] {gid}: webdriver error (attempt {attempt}/{MAX_RETRIES}) -> {type(e).__name__}\")\n",
    "                time.sleep(min(2 ** attempt, 8))\n",
    "            else:\n",
    "                log(f\"[FAIL] {gid}: non-retryable webdriver error -> {type(e).__name__}: {e}\")\n",
    "                return None\n",
    "\n",
    "    log(f\"[SKIP] {gid}: failed after {MAX_RETRIES} retries -> {type(last_err).__name__ if last_err else 'Unknown'}\")\n",
    "    return None\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "games = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=True)\n",
    "for col in [\"boxscore_url\", \"winner_sid\", \"loser_sid\"]:\n",
    "    if col not in games.columns:\n",
    "        raise ValueError(f\"INPUT_CSV missing required column: {col}\")\n",
    "\n",
    "if \"gender\" not in games.columns:\n",
    "    games[\"gender\"] = \"women\"\n",
    "\n",
    "driver = init_driver(headless=HEADLESS)\n",
    "\n",
    "try:\n",
    "    for i, row in games.iterrows():\n",
    "        url = s(row.get(\"boxscore_url\"))\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        winner_sid = s(row.get(\"winner_sid\"))\n",
    "        loser_sid  = s(row.get(\"loser_sid\"))\n",
    "        gender     = (s(row.get(\"gender\")).lower() or \"women\")\n",
    "\n",
    "        # If one side is missing (e.g., opponent not in your SID table), scrape only the valid one.\n",
    "        valid_sids = [sid for sid in [winner_sid, loser_sid] if sid]\n",
    "\n",
    "        if len(valid_sids) == 0:\n",
    "            log(f\"[SKIP] row {i}: no valid sids for {url}\")\n",
    "            continue\n",
    "\n",
    "        gid = game_id_from_url(url)\n",
    "        suffix = \"_w\" if gender.startswith(\"w\") else \"\"\n",
    "\n",
    "        # Build table specs only for valid sids (2 tables per valid sid)\n",
    "        table_specs = []\n",
    "        for sid in valid_sids:\n",
    "            table_specs.append((f\"box-score-basic-{sid}{suffix}\",    f\"{gid}_{sid}_basic.csv\"))\n",
    "            table_specs.append((f\"box-score-advanced-{sid}{suffix}\", f\"{gid}_{sid}_advanced.csv\"))\n",
    "\n",
    "        # If all expected outputs for the valid sids already exist, skip\n",
    "        out_paths = [os.path.join(OUT_DIR, fn) for _, fn in table_specs]\n",
    "        if out_paths and all(os.path.exists(p) for p in out_paths):\n",
    "            continue\n",
    "\n",
    "        html = fetch_html_with_retries(driver, url, gid)\n",
    "        if html is None:\n",
    "            continue\n",
    "\n",
    "        for table_id, filename in table_specs:\n",
    "            out_path = os.path.join(OUT_DIR, filename)\n",
    "            if os.path.exists(out_path):\n",
    "                continue\n",
    "\n",
    "            df = extract_table_df(html, table_id)\n",
    "            if df is None:\n",
    "                log(f\"[MISS] {gid}: table not found -> #{table_id}\")\n",
    "                continue\n",
    "\n",
    "            df.to_csv(out_path, index=False)\n",
    "\n",
    "        log(f\"[OK] {gid} -> wrote {len(table_specs)} tables (for {len(valid_sids)} valid sid(s))\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique SIDs: 362\n",
      "Total games (paired, across all SIDs): 10076\n",
      "Saved audit CSV: /users/cooperfoster/desktop/w_hoops/boxscore_audit.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>games_with_any_file</th>\n",
       "      <th>games_with_both_files</th>\n",
       "      <th>games_missing_basic</th>\n",
       "      <th>games_missing_advanced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>butler</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>georgia-southern</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>james-madison</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>milwaukee</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>old-dominion</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>washington-state</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>arizona</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>arkansas</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>baylor</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>bellarmine</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>boston-college</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>california</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>chicago-state</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>cleveland-state</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>connecticut</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>depaul</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>georgia-state</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>gonzaga</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>green-bay</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>iowa-state</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sid  games_with_any_file  games_with_both_files  \\\n",
       "93             butler                   31                     31   \n",
       "2    georgia-southern                   31                     31   \n",
       "243     james-madison                   31                     31   \n",
       "226         milwaukee                   31                     31   \n",
       "317      old-dominion                   31                     31   \n",
       "39   washington-state                   31                     31   \n",
       "290           arizona                   30                     30   \n",
       "156          arkansas                   30                     30   \n",
       "23             baylor                   30                     30   \n",
       "313        bellarmine                   30                     30   \n",
       "85     boston-college                   30                     30   \n",
       "129        california                   30                     30   \n",
       "217     chicago-state                   30                     30   \n",
       "134   cleveland-state                   30                     30   \n",
       "297       connecticut                   30                     30   \n",
       "176            depaul                   30                     30   \n",
       "186     georgia-state                   30                     30   \n",
       "123           gonzaga                   30                     30   \n",
       "112         green-bay                   30                     30   \n",
       "124        iowa-state                   30                     30   \n",
       "\n",
       "     games_missing_basic  games_missing_advanced  \n",
       "93                     0                       0  \n",
       "2                      0                       0  \n",
       "243                    0                       0  \n",
       "226                    0                       0  \n",
       "317                    0                       0  \n",
       "39                     0                       0  \n",
       "290                    0                       0  \n",
       "156                    0                       0  \n",
       "23                     0                       0  \n",
       "313                    0                       0  \n",
       "85                     0                       0  \n",
       "129                    0                       0  \n",
       "217                    0                       0  \n",
       "134                    0                       0  \n",
       "297                    0                       0  \n",
       "176                    0                       0  \n",
       "186                    0                       0  \n",
       "123                    0                       0  \n",
       "112                    0                       0  \n",
       "124                    0                       0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==== EDIT THESE ====\n",
    "OUT_DIR   = \"/users/cooperfoster/desktop/w_hoops/box scores\"\n",
    "AUDIT_CSV = \"/users/cooperfoster/desktop/w_hoops/boxscore_audit.csv\"\n",
    "# ====================\n",
    "\n",
    "def parse_boxscore_filename(fname: str):\n",
    "    \"\"\"\n",
    "    Expected: {game_id}_{sid}_{basic|advanced}.csv\n",
    "    Parse from the right so game_id can contain underscores safely.\n",
    "    \"\"\"\n",
    "    if not fname.lower().endswith(\".csv\"):\n",
    "        return None\n",
    "    stem = fname[:-4]\n",
    "    parts = stem.rsplit(\"_\", 2)  # -> [game_id, sid, typ]\n",
    "    if len(parts) != 3:\n",
    "        return None\n",
    "    game_id, sid, typ = parts\n",
    "    if typ not in (\"basic\", \"advanced\"):\n",
    "        return None\n",
    "    if not game_id or not sid:\n",
    "        return None\n",
    "    return game_id, sid, typ\n",
    "\n",
    "# sid -> game_id -> set(types)\n",
    "sid_games = defaultdict(lambda: defaultdict(set))\n",
    "\n",
    "# Walk folder (handles nested dirs too)\n",
    "for root, _, files in os.walk(OUT_DIR):\n",
    "    for fname in files:\n",
    "        parsed = parse_boxscore_filename(fname)\n",
    "        if not parsed:\n",
    "            continue\n",
    "        game_id, sid, typ = parsed\n",
    "        sid_games[sid][game_id].add(typ)\n",
    "\n",
    "rows = []\n",
    "for sid, games in sid_games.items():\n",
    "    total_games_any = len(games)\n",
    "    paired_games = sum(1 for tset in games.values() if \"basic\" in tset and \"advanced\" in tset)\n",
    "    missing_basic = sum(1 for tset in games.values() if \"basic\" not in tset)\n",
    "    missing_advanced = sum(1 for tset in games.values() if \"advanced\" not in tset)\n",
    "\n",
    "    rows.append({\n",
    "        \"sid\": sid,\n",
    "        \"games_with_any_file\": total_games_any,\n",
    "        \"games_with_both_files\": paired_games,          # this is the â€œsuccessfulâ€ definition (basic+advanced)\n",
    "        \"games_missing_basic\": missing_basic,\n",
    "        \"games_missing_advanced\": missing_advanced\n",
    "    })\n",
    "\n",
    "audit = pd.DataFrame(rows).sort_values([\"games_with_both_files\", \"sid\"], ascending=[False, True])\n",
    "audit.to_csv(AUDIT_CSV, index=False)\n",
    "\n",
    "print(f\"Unique SIDs: {audit['sid'].nunique()}\")\n",
    "print(f\"Total games (paired, across all SIDs): {audit['games_with_both_files'].sum()}\")\n",
    "print(f\"Saved audit CSV: {AUDIT_CSV}\")\n",
    "audit.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20152 csv files\n",
      "Unparseable filenames: 0\n",
      "Unique games (game_id): 5221\n",
      "Unique sids: 362\n",
      "Total columns in master: 43\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 17 elements, new values have 16 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 140\u001b[39m\n\u001b[32m    137\u001b[39m     read_fail += \u001b[32m1\u001b[39m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m df = \u001b[43mclean_boxscore_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df.empty:\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mclean_boxscore_df\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     54\u001b[39m cols = standardize_header(df.columns)\n\u001b[32m     55\u001b[39m df = df.copy()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m = cols\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df.columns) == \u001b[32m0\u001b[39m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.iloc[\u001b[32m0\u001b[39m:\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/core/generic.py:6313\u001b[39m, in \u001b[36mNDFrame.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   6311\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   6312\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[32m-> \u001b[39m\u001b[32m6313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6314\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m   6315\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mproperties.pyx:69\u001b[39m, in \u001b[36mpandas._libs.properties.AxisProperty.__set__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/core/generic.py:814\u001b[39m, in \u001b[36mNDFrame._set_axis\u001b[39m\u001b[34m(self, axis, labels)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    810\u001b[39m \u001b[33;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[33;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    813\u001b[39m labels = ensure_index(labels)\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[38;5;28mself\u001b[39m._clear_item_cache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/core/internals/managers.py:238\u001b[39m, in \u001b[36mBaseBlockManager.set_axis\u001b[39m\u001b[34m(self, axis, new_labels)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    237\u001b[39m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28mself\u001b[39m.axes[axis] = new_labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.13/site-packages/pandas/core/internals/base.py:98\u001b[39m, in \u001b[36mDataManager._validate_set_axis\u001b[39m\u001b[34m(self, axis, new_labels)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m new_len != old_len:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     99\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elements, new \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elements\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Length mismatch: Expected axis has 17 elements, new values have 16 elements"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# ==== EDIT THESE ====\n",
    "IN_DIR     = \"/users/cooperfoster/desktop/w_hoops/box scores\"   # folder with ~20k per-team boxscore CSVs\n",
    "OUT_CSV    = \"/users/cooperfoster/desktop/w_hoops/master_boxscore\"     # output master CSV\n",
    "# ====================\n",
    "\n",
    "# Expected filename pattern from your scraper:\n",
    "#   {game_id}_{sid}_{basic|advanced}.csv\n",
    "# game_id often starts with YYYY-MM-DD-...\n",
    "fname_re = re.compile(r\"^(?P<game_id>.+)_(?P<sid>[^_]+)_(?P<typ>basic|advanced)\\.csv$\", re.IGNORECASE)\n",
    "date_re  = re.compile(r\"^(?P<date>\\d{4}-\\d{2}-\\d{2})\\b\")\n",
    "\n",
    "def parse_meta_from_fname(fname):\n",
    "    m = fname_re.match(fname)\n",
    "    if not m:\n",
    "        return None\n",
    "    game_id = m.group(\"game_id\")\n",
    "    sid     = m.group(\"sid\")\n",
    "    typ     = m.group(\"typ\").lower()\n",
    "\n",
    "    dm = date_re.match(game_id)\n",
    "    game_date = dm.group(\"date\") if dm else \"\"\n",
    "\n",
    "    return game_id, game_date, sid, typ\n",
    "\n",
    "def standardize_header(cols):\n",
    "    cols = [str(c) for c in cols]\n",
    "    # drop common junk\n",
    "    cols = [c for c in cols if c.lower() not in (\"rk\",) and not c.lower().startswith(\"unnamed:\")]\n",
    "    if len(cols) == 0:\n",
    "        return cols\n",
    "    # normalize first column name to \"player\"\n",
    "    cols[0] = \"player\"\n",
    "    # make unique\n",
    "    out, seen = [], {}\n",
    "    for c in cols:\n",
    "        c = c.strip()\n",
    "        if c not in seen:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "        else:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}.{seen[c]}\")\n",
    "    return out\n",
    "\n",
    "def clean_boxscore_df(df):\n",
    "    # standardize cols\n",
    "    cols = standardize_header(df.columns)\n",
    "    df = df.copy()\n",
    "    df.columns = cols\n",
    "\n",
    "    if len(df.columns) == 0:\n",
    "        return df.iloc[0:0]\n",
    "\n",
    "    # Ensure player column exists\n",
    "    if \"player\" not in df.columns:\n",
    "        df.rename(columns={df.columns[0]: \"player\"}, inplace=True)\n",
    "\n",
    "    # Drop separator/header rows like \"Starters\" and \"Reserves\"\n",
    "    # (shown in your example boxscore CSV)\n",
    "    df[\"player\"] = df[\"player\"].astype(str).str.strip()\n",
    "    df = df[~df[\"player\"].isin([\"Starters\", \"Reserves\"])]\n",
    "\n",
    "    # Also drop any repeated header rows that can sneak in (player == column name)\n",
    "    df = df[df[\"player\"].str.lower() != \"player\"]\n",
    "\n",
    "    # Drop fully empty rows\n",
    "    df.replace(\"\", pd.NA, inplace=True)\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    return df\n",
    "\n",
    "# ---------- PASS 1: scan filenames (gid->sids) + collect union of columns ----------\n",
    "paths = sorted(glob.glob(os.path.join(IN_DIR, \"*.csv\")))\n",
    "gid_to_sids = defaultdict(set)\n",
    "all_cols = set()\n",
    "\n",
    "bad_name = 0\n",
    "for p in paths:\n",
    "    meta = parse_meta_from_fname(os.path.basename(p))\n",
    "    if not meta:\n",
    "        bad_name += 1\n",
    "        continue\n",
    "    gid, gdate, sid, typ = meta\n",
    "    gid_to_sids[gid].add(sid)\n",
    "\n",
    "    # read just header fast\n",
    "    try:\n",
    "        hdr = pd.read_csv(p, nrows=0).columns\n",
    "        hdr_std = standardize_header(hdr)\n",
    "        for c in hdr_std:\n",
    "            all_cols.add(c)\n",
    "    except Exception:\n",
    "        # ignore header read failures here; we'll catch on full read\n",
    "        pass\n",
    "\n",
    "# metadata columns we add\n",
    "meta_cols = [\"game_id\", \"game_date\", \"sid\", \"opp_sid\", \"table_type\"]\n",
    "data_cols = [c for c in sorted(all_cols) if c != \"player\"]  # keep player first\n",
    "final_cols = meta_cols + [\"player\"] + data_cols\n",
    "\n",
    "print(f\"Found {len(paths)} csv files\")\n",
    "print(f\"Unparseable filenames: {bad_name}\")\n",
    "print(f\"Unique games (game_id): {len(gid_to_sids)}\")\n",
    "print(f\"Unique sids: {len({s for ss in gid_to_sids.values() for s in ss})}\")\n",
    "print(f\"Total columns in master: {len(final_cols)}\")\n",
    "\n",
    "# ---------- PASS 2: read, clean, add metadata, write incrementally ----------\n",
    "os.makedirs(os.path.dirname(OUT_CSV) or \".\", exist_ok=True)\n",
    "\n",
    "first_write = True\n",
    "read_fail = 0\n",
    "rows_written = 0\n",
    "\n",
    "for idx, p in enumerate(paths, start=1):\n",
    "    fname = os.path.basename(p)\n",
    "    meta = parse_meta_from_fname(fname)\n",
    "    if not meta:\n",
    "        continue\n",
    "\n",
    "    gid, gdate, sid, typ = meta\n",
    "\n",
    "    # opponent inference: other sid in the same game_id (if exactly 2 known)\n",
    "    sids_in_game = sorted(gid_to_sids.get(gid, []))\n",
    "    opp = \"\"\n",
    "    if len(sids_in_game) == 2:\n",
    "        opp = sids_in_game[1] if sids_in_game[0] == sid else sids_in_game[0]\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(p, dtype=str, keep_default_na=False)\n",
    "    except Exception:\n",
    "        read_fail += 1\n",
    "        continue\n",
    "\n",
    "    df = clean_boxscore_df(df)\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    # add metadata\n",
    "    df.insert(0, \"table_type\", typ)\n",
    "    df.insert(0, \"opp_sid\", opp)\n",
    "    df.insert(0, \"sid\", sid)\n",
    "    df.insert(0, \"game_date\", gdate)\n",
    "    df.insert(0, \"game_id\", gid)\n",
    "\n",
    "    # align to final schema\n",
    "    for c in final_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "    df = df[final_cols]\n",
    "\n",
    "    df.to_csv(OUT_CSV, mode=\"w\" if first_write else \"a\", index=False, header=first_write)\n",
    "    first_write = False\n",
    "    rows_written += len(df)\n",
    "\n",
    "print(f\"Master written: {OUT_CSV}\")\n",
    "print(f\"Rows written: {rows_written}\")\n",
    "print(f\"Read failures: {read_fail}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: 20152 | Unparseable filenames: 0 | Unique games: 5221\n",
      "Processed 1000/20152 files | rows written: 11954\n",
      "Processed 2000/20152 files | rows written: 23666\n",
      "Processed 3000/20152 files | rows written: 35296\n",
      "Processed 4000/20152 files | rows written: 46840\n",
      "Processed 5000/20152 files | rows written: 58322\n",
      "Processed 6000/20152 files | rows written: 69614\n",
      "Processed 7000/20152 files | rows written: 81094\n",
      "Processed 8000/20152 files | rows written: 92436\n",
      "Processed 9000/20152 files | rows written: 103758\n",
      "Processed 10000/20152 files | rows written: 114864\n",
      "Processed 11000/20152 files | rows written: 125904\n",
      "Processed 12000/20152 files | rows written: 136848\n",
      "Processed 13000/20152 files | rows written: 147888\n",
      "Processed 14000/20152 files | rows written: 158784\n",
      "Processed 15000/20152 files | rows written: 169572\n",
      "Processed 16000/20152 files | rows written: 180428\n",
      "Processed 17000/20152 files | rows written: 191272\n",
      "Processed 18000/20152 files | rows written: 202112\n",
      "Processed 19000/20152 files | rows written: 212788\n",
      "Processed 20000/20152 files | rows written: 223548\n",
      "Done. Master CSV: /users/cooperfoster/desktop/w_hoops/master_boxscore.csv\n",
      "Rows written: 225218 | read failures: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# ==== EDIT THESE ====\n",
    "IN_DIR     = \"/users/cooperfoster/desktop/w_hoops/box scores\"   # folder with ~20k per-team boxscore CSVs\n",
    "OUT_CSV    = \"/users/cooperfoster/desktop/w_hoops/master_boxscore.csv\"  \n",
    "# ====================\n",
    "\n",
    "# filename: {game_id}_{sid}_{basic|advanced}.csv  (parse from the right)\n",
    "fname_re = re.compile(r\"^(?P<game_id>.+)_(?P<sid>[^_]+)_(?P<typ>basic|advanced)\\.csv$\", re.IGNORECASE)\n",
    "date_re  = re.compile(r\"^(?P<date>\\d{4}-\\d{2}-\\d{2})\")\n",
    "\n",
    "DROP_PLAYER_ROWS = {\"Starters\", \"Reserves\"}  # add \"School Totals\" here if you want to drop totals too\n",
    "\n",
    "def parse_meta(fname):\n",
    "    m = fname_re.match(fname)\n",
    "    if not m:\n",
    "        return None\n",
    "    gid = m.group(\"game_id\")\n",
    "    sid = m.group(\"sid\")\n",
    "    typ = m.group(\"typ\").lower()\n",
    "    dm = date_re.match(gid)\n",
    "    gdate = dm.group(\"date\") if dm else \"\"\n",
    "    return gid, gdate, sid, typ\n",
    "\n",
    "def make_unique(cols):\n",
    "    out, seen = [], {}\n",
    "    for c in cols:\n",
    "        c = str(c).strip()\n",
    "        if c not in seen:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "        else:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}.{seen[c]}\")\n",
    "    return out\n",
    "\n",
    "def read_boxscore_csv(path):\n",
    "    \"\"\"\n",
    "    Most of your files are written with a 2-line header:\n",
    "      row0: 'Basic Box Score Stats ...'\n",
    "      row1: 'Starters,MP,FG,...'\n",
    "    So we read header=1 by default, and fall back if needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=1, dtype=str, keep_default_na=False)\n",
    "        # sanity check: we expect MP to exist in both basic & advanced tables\n",
    "        if \"MP\" in df.columns:\n",
    "            return df\n",
    "    except Exception:\n",
    "        pass\n",
    "    return pd.read_csv(path, dtype=str, keep_default_na=False)\n",
    "\n",
    "def clean_df(df):\n",
    "    df = df.copy()\n",
    "    df.columns = make_unique(df.columns)\n",
    "\n",
    "    # normalize first column name to player (it's named \"Starters\" in the header line)\n",
    "    if len(df.columns) == 0:\n",
    "        return df.iloc[0:0]\n",
    "    df.rename(columns={df.columns[0]: \"player\"}, inplace=True)\n",
    "\n",
    "    # drop section header rows\n",
    "    df[\"player\"] = df[\"player\"].astype(str).str.strip()\n",
    "    df = df[~df[\"player\"].isin(DROP_PLAYER_ROWS)]\n",
    "\n",
    "    # drop fully empty rows\n",
    "    df.replace(\"\", pd.NA, inplace=True)\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    return df\n",
    "\n",
    "# ---- gather all file paths ----\n",
    "paths = []\n",
    "for root, _, files in os.walk(IN_DIR):\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".csv\"):\n",
    "            paths.append(os.path.join(root, f))\n",
    "paths.sort()\n",
    "\n",
    "# ---- PASS 1: build gid->sids and union of columns ----\n",
    "gid_to_sids = defaultdict(set)\n",
    "union_cols = set()\n",
    "bad_names = 0\n",
    "\n",
    "for p in paths:\n",
    "    meta = parse_meta(os.path.basename(p))\n",
    "    if not meta:\n",
    "        bad_names += 1\n",
    "        continue\n",
    "    gid, gdate, sid, typ = meta\n",
    "    gid_to_sids[gid].add(sid)\n",
    "\n",
    "    # grab header quickly\n",
    "    try:\n",
    "        hdr = pd.read_csv(p, header=1, nrows=0).columns\n",
    "        hdr = list(hdr)\n",
    "        if hdr:\n",
    "            hdr[0] = \"player\"\n",
    "        for c in make_unique(hdr):\n",
    "            union_cols.add(c)\n",
    "    except Exception:\n",
    "        # fallback: try header=0\n",
    "        try:\n",
    "            hdr = list(pd.read_csv(p, nrows=0).columns)\n",
    "            if hdr:\n",
    "                hdr[0] = \"player\"\n",
    "            for c in make_unique(hdr):\n",
    "                union_cols.add(c)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "meta_cols = [\"game_id\", \"game_date\", \"team\", \"opponent\", \"table_type\"]\n",
    "data_cols = [\"player\"] + sorted(c for c in union_cols if c != \"player\")\n",
    "final_cols = meta_cols + data_cols\n",
    "\n",
    "print(f\"Files found: {len(paths)} | Unparseable filenames: {bad_names} | Unique games: {len(gid_to_sids)}\")\n",
    "\n",
    "# ---- PASS 2: clean + write incrementally ----\n",
    "first_write = True\n",
    "rows_written = 0\n",
    "read_fail = 0\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_CSV) or \".\", exist_ok=True)\n",
    "\n",
    "for k, p in enumerate(paths, start=1):\n",
    "    meta = parse_meta(os.path.basename(p))\n",
    "    if not meta:\n",
    "        continue\n",
    "    gid, gdate, sid, typ = meta\n",
    "\n",
    "    # opponent inferred from other sid in same game_id (if present)\n",
    "    sids_in_game = sorted(gid_to_sids.get(gid, []))\n",
    "    opp = \"\"\n",
    "    if len(sids_in_game) == 2:\n",
    "        opp = sids_in_game[1] if sids_in_game[0] == sid else sids_in_game[0]\n",
    "\n",
    "    try:\n",
    "        df = read_boxscore_csv(p)\n",
    "    except Exception:\n",
    "        read_fail += 1\n",
    "        continue\n",
    "\n",
    "    df = clean_df(df)\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    # add metadata\n",
    "    df.insert(0, \"table_type\", typ)\n",
    "    df.insert(0, \"opponent\", opp)\n",
    "    df.insert(0, \"team\", sid)\n",
    "    df.insert(0, \"game_date\", gdate)\n",
    "    df.insert(0, \"game_id\", gid)\n",
    "\n",
    "    # align columns\n",
    "    for c in final_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "    df = df[final_cols]\n",
    "\n",
    "    df.to_csv(OUT_CSV, mode=\"w\" if first_write else \"a\", index=False, header=first_write)\n",
    "    first_write = False\n",
    "    rows_written += len(df)\n",
    "\n",
    "    if k % 1000 == 0:\n",
    "        print(f\"Processed {k}/{len(paths)} files | rows written: {rows_written}\")\n",
    "\n",
    "print(f\"Done. Master CSV: {OUT_CSV}\")\n",
    "print(f\"Rows written: {rows_written} | read failures: {read_fail}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
